{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2011dd84",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25457fe",
   "metadata": {},
   "source": [
    "This script implements a real-time facial emotion recognition application using a pre-trained deep learning model.\n",
    "It captures video from the default camera (webcam) and detects faces in the frames using the Haar Cascade classifier.\n",
    "For each detected face, the script predicts the corresponding emotion (e.g., Angry, Happy) based on a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac6cb5",
   "metadata": {},
   "source": [
    "In this code, a class FacialExpressionModel is defined for predicting facial expressions using a loaded neural network model. It has methods for initialization and emotion prediction. The class attribute EMOTIONS_LIST holds the emotion labels corresponding to the model's output classes. The init method loads the model architecture and weights from provided files. The predict_emotion method takes an input image, makes predictions using the loaded model, and returns the predicted emotion label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34371033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 23:34:14.361498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import os \n",
    "os.chdir('/Users/clementine/Desktop/Maths Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e315d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for facial expression prediction using a loaded neural network model\n",
    "class FacialExpressionModel(object):\n",
    "    # List of emotion labels corresponding to model output classes\n",
    "    EMOTIONS_LIST = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "    # Initialize the class with a trained model's JSON and weights files\n",
    "    def __init__(self, model_json_file, model_weights_file):\n",
    "        with open(model_json_file, \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            self.loaded_model = model_from_json(loaded_model_json)\n",
    "            \n",
    "        # Load the model weights from the weights file\n",
    "        self.loaded_model.load_weights(model_weights_file)\n",
    "        print(\"Model loaded from disk\")\n",
    "        # Display a summary of the loaded model architecture\n",
    "        self.loaded_model.summary()\n",
    "        \n",
    "     # Predict emotion label for an input image\n",
    "    def predict_emotion(self, img):\n",
    "        # Make predictions using the loaded model\n",
    "        self.preds = self.loaded_model.predict(img)\n",
    "        \n",
    "         # Return the emotion label with the highest predicted probability\n",
    "        return FacialExpressionModel.EMOTIONS_LIST[np.argmax(self.preds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dac5b9",
   "metadata": {},
   "source": [
    "Below code forms a real-time facial emotion recognition application using computer vision techniques and a pre-trained deep learning model. By accessing the camera feed, the application captures video frames, detects faces, and predicts the emotions depicted by individuals in real time. It overlays the predicted emotion labels on detected faces and outlines the faces with rectangles. The code showcases the integration of a loaded emotion prediction model, Haar Cascade face detection, and OpenCV visualization to create an interactive and informative tool for recognizing emotions from live video streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f213d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk\n",
      "Model: \"DCNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 48, 48, 64)        1664      \n",
      "                                                                 \n",
      " block1_batchnorm1 (BatchNor  (None, 48, 48, 64)       256       \n",
      " malization)                                                     \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 48, 48, 64)        102464    \n",
      "                                                                 \n",
      " block1_batchnorm2 (BatchNor  (None, 48, 48, 64)       256       \n",
      " malization)                                                     \n",
      "                                                                 \n",
      " block1_maxpool (MaxPooling2  (None, 24, 24, 64)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " block1_dropout (Dropout)    (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " block2_batchnorm1 (BatchNor  (None, 24, 24, 128)      512       \n",
      " malization)                                                     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " block2_batchnorm2 (BatchNor  (None, 24, 24, 128)      512       \n",
      " malization)                                                     \n",
      "                                                                 \n",
      " block2_maxpool (MaxPooling2  (None, 12, 12, 128)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " block2_dropout (Dropout)    (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " block3_batchnorm1 (BatchNor  (None, 12, 12, 256)      1024      \n",
      " malization)                                                     \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_batchnorm2 (BatchNor  (None, 12, 12, 256)      1024      \n",
      " malization)                                                     \n",
      "                                                                 \n",
      " block3_maxpool (MaxPooling2  (None, 6, 6, 256)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " block3_dropout (Dropout)    (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               1179776   \n",
      "                                                                 \n",
      " dense_batchnorm (BatchNorma  (None, 128)              512       \n",
      " lization)                                                       \n",
      "                                                                 \n",
      " dense_dropout (Dropout)     (None, 128)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,395,591\n",
      "Trainable params: 2,393,543\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "# Initialize the camera capture object\n",
    "cap = cap = cv2.VideoCapture(0)\n",
    "# Load the Haar Cascade classifier for detecting faces\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# Define the font style for text overlay on the video frames\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Function to capture video frames, detect faces, and return necessary data\n",
    "def getdata():\n",
    "    _, fr = cap.read() # Read a frame from the camera\n",
    "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY) # Convert the frame to grayscale\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.3, 5)  # Detect faces using the Haar Cascade classifier\n",
    "    return faces, fr, gray\n",
    "\n",
    "# Function to start the facial emotion recognition application\n",
    "def start_app(cnn):\n",
    "    while cap.isOpened(): \n",
    "        faces, fr, gray_fr = getdata() # Capture data from the camera\n",
    "        for (x, y, w, h) in faces: \n",
    "            fc = gray_fr[y:y + h, x:x + w] # Extract the face region\n",
    "            roi = cv2.resize(fc, (48, 48)) # Resize the face region for prediction\n",
    "            pred = cnn.predict_emotion(roi[np.newaxis, :, :, np.newaxis]) # Predict the emotion\n",
    "            \n",
    "            # Overlay the predicted emotion label and draw a rectangle around the face\n",
    "            cv2.putText(fr, pred, (x, y), font, 1, (255, 255, 0), 1)\n",
    "            cv2.rectangle(fr, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
    "        \n",
    "        # Exit the application if the 'Esc' key is pressed\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "        cv2.imshow('Facial Emotion Recognition', fr) # Display the frame with overlays\n",
    "        \n",
    "    # Release the camera and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__': \n",
    "    model = FacialExpressionModel(\"model.json\", \"model.h5\") # Load the emotion prediction model\n",
    "    start_app(model)  # Start the real-time emotion recognition application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
